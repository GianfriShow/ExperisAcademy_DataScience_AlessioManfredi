{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 10:45:33.907419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import models,layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 2\n",
    "\n",
    "'''\n",
    "Linear Regression - metodo semplice che spiega una variabile continua di target in base a delle features; semplice e veloce da usare ed implementare, facile anche da interpretare siccome spiega l'impatto che ogni variabile ha sul target finale tramite il corrispondente coefficiente. Graficamente va a \"fittare\" un hyperplane di predizione che cerca di avvicinarsi ai punti di training il più possibile minimizzando il MSE.\n",
    "\n",
    "(Multinomial) Logistic Regression - nella versione più semplice e' simile alla linear regression ma applica una sigmoid curve all'output per forzarlo in un range che sta tra 0 a 1 asintoticamente, rendendo il modello utile alla classificazione binaria (esiste anche il multinomial logit per più classi).\n",
    "\n",
    "K-Nearest Neighbours (KNN) - modello molto semplice e particolare perché non si allena nel vero senso del termine ma va solo a fare predictions su nuovi punti basandosi su quelli più vicini as essi tra quelli che il modello ha fino ad ora visto; il parametro fondamentale è il numero di vicini da considerare (k), e il tipo di distanza che viene considerata per attribuire i neighbours (solitamente la classica Euclidean distance).\n",
    "\n",
    "K-Means Clustering - modello che inizializza dei centroidi casuali (oppure dati dall'utente se ha informazioni utili, o anche in modo intelligente da un algoritmo) per un numero 'k' di clusters, poi calcola la distanza (Euclidean di default) di tutti i punti dai centroidi, assegna i punti al cluster il cui centroide è più vicino, infine ricalcola i centroidi facendo la media di tutti i punti che ora si trovano in quel cluster, per poi ripetere il processo recursively fino a che gli assegnamenti non cambiano più oppure si raggiunge un limite di iterazioni fornito dall'utente. Questo algoritmo di classificazione è vulnerabile a partenze di centroidi non ottimali, per questo ha bisogno di essere ripetuto molte volte per cercare di trovare una soluzione ottimale che minimizza la varianza interna ai clusters e massimizza quella esterna (vogliamo clusters il più internamente omogenei ed esternamente eterogenei possibile).\n",
    "\n",
    "Decision Tree - modello per problemi di classificazione basato su una struttura decisionale ad albero greedy che va a cercare iterativamente la migliore classe su cui fare uno split che ottimizza una data funzione come per esempio il coefficiente di Gini o l'entropia. L'obiettivo dello split è quello di dividere i punti in modo tale da ridurre l'entropia del sistema cioè lo stato di confusione, quindi vogliamo separare i punti in modo tale che più punti della stessa classe siano dallo stesso lato ogni volta, per ogni classe. Possiamo regolarizzare decidendo di considerare solo un subset casuale delle features ad ogni split (lo standard è un numero pari alla radice quadrata del numero di features totali), il che renderà l'albero meno dipendente da certe features dominanti e lo porterà a considerarle tutte in modo più equilibrato.\n",
    "\n",
    "Random Forest - basato sui decision trees, il random forest aiuta a ridurre l'overfitting e la variabilità delle predictions creando molti decision trees e facendo una media delle loro predictions oppure una votazione a maggioranza.\n",
    "\n",
    "Support Vector Machine (SVM) - modello un po' più complicato dei precedenti che va a considerare lo spazio dimensionale in cui si trovano i dati e cerca di tracciare un hyperplane che divide nel modo più ottimale i punti per classificarli, massimizzando la distanza dai punti più vicini a questo hyperplane. Può avere un parametro che determina la tolleranza di quanti punti possono trovarsi dal lato sbagliato della classificazione, il che può permettere di migliorare la distanza dai support vectors (punti più vicini).\n",
    "\n",
    "Linear Discriminant Analysis (LDA) - anche questo modello cerca di trovare i confini migliori per separare i dati nel modo ottimale in un problema di classificazione.\n",
    "\n",
    "Naive Bayes - \n",
    "\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 #\n",
    "\n",
    "Descrizione:  Crea un programma in Python per gestire una semplice libreria di libri. Il programma dovrebbe presentare un menu all'utente con le seguenti opzioni:\n",
    "- Aggiungere un nuovo libro: L'utente può inserire il titolo, l'autore e l'anno di pubblicazione del libro e quantità.\n",
    "- Visualizzare tutti i libri: Mostra una lista di tutti i libri attualmente nella libreria.\n",
    "- Cercare un libro per titolo: L'utente inserisce un titolo e il programma cerca e mostra i dettagli del libro se trovato.\n",
    "- Gestione libri: Far rimuovere modificare e/o aggiungere una compia in più del libro\n",
    "- Uscire dal programma: Termina l'esecuzione del programma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 5\n",
    "\n",
    "'''\n",
    "Il Machine Learning è una strategia generale di apprendimento per le macchine che permette loro di apprendere certi tasks attraverso l'analisi di dati d'allenamento e l'ottimizzazione di certe funzioni (minimizzando una loss function solitamente).\n",
    "Possiamo distinguere varie fasi solitamente:\n",
    "- Caricamento dataset\n",
    "- Analisi dataset + conseguente pulizia in base ai risultati dell'analisi e obiettivi della ricerca\n",
    "- Split dataset in allenamento+validazione+test\n",
    "- Scelta, definizione, compilazione modello\n",
    "- Allenamento modello con validazione durante il fit\n",
    "- Test modello sul test dataset, e in base ai risultati possibilmente si ritorna a steps precedenti\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
